{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08e91cdb-4693-4632-b7aa-f37eec027131",
   "metadata": {},
   "source": [
    "## Working with Transformers in the HuggingFace Ecosystem\n",
    "\n",
    "In this laboratory exercise we will learn how to work with the HuggingFace ecosystem to adapt models to new tasks. As you will see, much of what is required is *investigation* into the inner-workings of the HuggingFace abstractions. With a little work, a little trial-and-error, it is fairly easy to get a working adaptation pipeline up and running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e556105-269f-43e3-8933-227269afb9ea",
   "metadata": {},
   "source": [
    "#### Exercise 1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from datasets import load_dataset, get_dataset_split_names\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import LinearSVC\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from collections import Counter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T14:21:39.977456400Z",
     "start_time": "2025-08-31T14:21:29.628203700Z"
    }
   },
   "id": "e7e3b2d4f225ef24"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15819af4-e850-412b-ab67-61b9b98e3a90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T13:24:08.771230900Z",
     "start_time": "2025-08-31T13:24:04.194665400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: ['train', 'validation', 'test']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
    "\n",
    "splits = get_dataset_split_names(\"cornell-movie-review-data/rotten_tomatoes\")\n",
    "print(\"Splits:\", splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8530\n",
      "Validation size: 1066\n",
      "Test size: 1066\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size:\", len(dataset[\"train\"]))\n",
    "print(\"Validation size:\", len(dataset[\"validation\"]))\n",
    "print(\"Test size:\", len(dataset[\"test\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:24:20.086297Z",
     "start_time": "2025-08-31T13:24:20.043805500Z"
    }
   },
   "id": "5ef46c69e90ec35"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      " {'text': '[a] rare , beautiful film .', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example:\\n\", dataset[\"train\"][211])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:24:27.267875500Z",
     "start_time": "2025-08-31T13:24:27.227455400Z"
    }
   },
   "id": "f4e3c07bfd064cc3"
  },
  {
   "cell_type": "markdown",
   "id": "a27f50a0-0af4-4bdd-9a60-bab26f126c14",
   "metadata": {},
   "source": [
    "#### Exercise 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eefc651-8551-44c2-8340-37d64660fc58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T13:24:34.611003500Z",
     "start_time": "2025-08-31T13:24:33.543512400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intensely romantic , thought-provoking and even an engaging mystery .\n",
      "{'input_ids': tensor([[  101, 20531,  6298,  1010,  2245,  1011,  4013, 22776,  1998,  2130,\n",
      "          2019, 11973,  6547,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModel.from_pretrained(name)\n",
    "\n",
    "sample = dataset[\"test\"][\"text\"][260]\n",
    "print(sample)\n",
    "\n",
    "encoded_input = tokenizer(sample, return_tensors=\"pt\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.2474, -0.2550, -0.1434,  ..., -0.2345,  0.4026,  0.1096],\n",
      "         [-0.0094,  0.2953,  0.0051,  ..., -0.3797,  0.2477, -0.3513],\n",
      "         [-0.2699,  0.1544,  0.3303,  ..., -0.5992,  0.1053, -0.4761],\n",
      "         ...,\n",
      "         [-0.1478, -0.0413,  0.4556,  ..., -0.2515,  0.2923, -0.3443],\n",
      "         [-0.5207, -0.7747, -0.1066,  ...,  0.3429,  0.4019, -0.4375],\n",
      "         [ 0.9744, -0.0602, -0.1639,  ..., -0.0061, -0.3364, -0.0374]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "output = model(**encoded_input)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:24:36.530811300Z",
     "start_time": "2025-08-31T13:24:36.234513200Z"
    }
   },
   "id": "a38cbd3d3042e9d9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Last hidden state shape: torch.Size([1, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLast hidden state shape:\", output.last_hidden_state.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:24:39.823147Z",
     "start_time": "2025-08-31T13:24:39.781649100Z"
    }
   },
   "id": "27f76d3348856a91"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a soul-stirring documentary about the israeli/palestinian conflict as revealed through the eyes of some children who remain curious about each other against all odds .\n",
      "\n",
      "Last hidden state shape: torch.Size([1, 31, 768])\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"test\"][\"text\"][29]\n",
    "print(sample)\n",
    "\n",
    "encoded_input = tokenizer(sample, return_tensors=\"pt\")\n",
    "\n",
    "output = model(**encoded_input)\n",
    "print(\"\\nLast hidden state shape:\", output.last_hidden_state.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-30T16:33:00.183219400Z",
     "start_time": "2025-08-30T16:33:00.085531600Z"
    }
   },
   "id": "832ce7f8c57deef4"
  },
  {
   "cell_type": "markdown",
   "id": "ec0f6fd4-b80b-466d-b4ff-9aac0492c062",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: A Stable Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb961ab-aa24-4d5c-86a1-c9b4237b40fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T16:33:00.184221200Z",
     "start_time": "2025-08-30T16:33:00.130554300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[ 101, 1045, 2215, 2000, 2156, 2065,  101, 2003, 2012, 1996, 2927, 2030,\n",
      "         2012, 1996, 2203, 1997, 1996, 5537,  102]])\n",
      "Decoded tokens: ['[CLS]', 'i', 'want', 'to', 'see', 'if', '[CLS]', 'is', 'at', 'the', 'beginning', 'or', 'at', 'the', 'end', 'of', 'the', 'sequence', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sample = \"i want to see if [CLS] is at the beginning or at the end of the sequence\"\n",
    "encoded = tokenizer(sample, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Token IDs:\", encoded[\"input_ids\"])\n",
    "print(\"Decoded tokens:\", tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### So the [CLS] is the first element of the output sequence!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cea50936cfa490"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def extract_features(texts, tokenizer, model, device=\"cuda\"):\n",
    "    model = model.to(device)\n",
    "    model.eval()  \n",
    "\n",
    "    hidden_size = 768  \n",
    "    features = torch.zeros((len(texts), hidden_size))\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for i, text in enumerate(tqdm(texts, desc=\"Extracting features\")):\n",
    "            encoded = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            output = model(**encoded)\n",
    "\n",
    "            cls_vec = output.last_hidden_state[:, 0, :] \n",
    "\n",
    "            features[i, :] = cls_vec.cpu()\n",
    "\n",
    "    return features\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-30T16:33:00.185221200Z",
     "start_time": "2025-08-30T16:33:00.141334Z"
    }
   },
   "id": "d53885d4b949b162"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 8530/8530 [01:08<00:00, 125.34it/s]\n",
      "Extracting features: 100%|██████████| 1066/1066 [00:08<00:00, 128.84it/s]\n",
      "Extracting features: 100%|██████████| 1066/1066 [00:08<00:00, 126.44it/s]\n"
     ]
    }
   ],
   "source": [
    "text_train = extract_features(dataset[\"train\"][\"text\"], tokenizer, model)\n",
    "labels_train = dataset[\"train\"][\"label\"]\n",
    "\n",
    "text_validation = extract_features(dataset[\"validation\"][\"text\"], tokenizer, model)\n",
    "labels_validation = dataset[\"validation\"][\"label\"]\n",
    "\n",
    "text_test = extract_features(dataset[\"test\"][\"text\"], tokenizer, model)\n",
    "labels_test = dataset[\"test\"][\"label\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-30T16:34:26.408656400Z",
     "start_time": "2025-08-30T16:33:00.145747700Z"
    }
   },
   "id": "d73ba30cb3617623"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.822\n",
      "Test accuracy: 0.798\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(text_train, labels_train)\n",
    "\n",
    "validation_predictions = clf.predict(text_validation)\n",
    "test_predictions = clf.predict(text_test)\n",
    "\n",
    "print(f\"Validation accuracy: {accuracy_score(labels_validation, validation_predictions):.3f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(labels_test, test_predictions):.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-30T16:34:32.009205300Z",
     "start_time": "2025-08-30T16:34:26.408656400Z"
    }
   },
   "id": "6da420f82da3854f"
  },
  {
   "cell_type": "markdown",
   "id": "37141d1b-935b-425c-804c-b9b487853791",
   "metadata": {},
   "source": [
    "-----\n",
    "### Exercise 2: Fine-tuning Distilbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392b1ed-597b-4a92-90fc-10eb11eac515",
   "metadata": {},
   "source": [
    "#### Exercise 2.1: Token Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e6e2a95-5b08-4f81-b824-59a0fb3404e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T16:34:32.021713100Z",
     "start_time": "2025-08-30T16:34:32.012205100Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(dataset_element):\n",
    "    encoded = tokenizer(dataset_element[\"text\"])\n",
    "\n",
    "    return {\n",
    "        \"text\": dataset_element[\"text\"],\n",
    "        \"label\": dataset_element[\"label\"],\n",
    "        \"input_ids\": encoded[\"input_ids\"],\n",
    "        \"attention_mask\": encoded[\"attention_mask\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "tokenized_train = dataset[\"train\"].map(tokenize)\n",
    "tokenized_val = dataset[\"validation\"].map(tokenize)\n",
    "tokenized_test = dataset[\"test\"].map(tokenize)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-30T16:34:32.084572600Z",
     "start_time": "2025-08-30T16:34:32.018713600Z"
    }
   },
   "id": "7c7e18b26df57c90"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'a soul-stirring documentary about the israeli/palestinian conflict as revealed through the eyes of some children who remain curious about each other against all odds .', 'label': 1, 'input_ids': [101, 1037, 3969, 1011, 18385, 4516, 2055, 1996, 5611, 1013, 9302, 4736, 2004, 3936, 2083, 1996, 2159, 1997, 2070, 2336, 2040, 3961, 8025, 2055, 2169, 2060, 2114, 2035, 10238, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_test[29])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-30T16:34:32.085606400Z",
     "start_time": "2025-08-30T16:34:32.051502900Z"
    }
   },
   "id": "1ff77ced3604c97f"
  },
  {
   "cell_type": "markdown",
   "id": "62a80de2-83c9-4c12-af4e-34babe23ffd1",
   "metadata": {},
   "source": [
    "#### Exercise 2.2: Setting up the Model to be Fine-tuned\n",
    "\n",
    "In this exercise we need to prepare the base Distilbert model for fine-tuning for a *sequence classification task*. This means, at the very least, appending a new, randomly-initialized classification head connected to the `[CLS]` token of the last transformer layer. Luckily, HuggingFace already provides an `AutoModel` for just this type of instantiation: [`AutoModelForSequenceClassification`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification). You will want you instantiate one of these for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6327d73-3b71-478a-9932-bb062a650c57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T16:34:32.256780500Z",
     "start_time": "2025-08-30T16:34:32.057066900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109f7bd-955f-4fc4-bc3e-75bd99a17adf",
   "metadata": {},
   "source": [
    "#### Exercise 2.3: Fine-tuning Distilbert\n",
    "\n",
    "Finally. In this exercise you should use a HuggingFace [`Trainer`](https://huggingface.co/docs/transformers/main/en/trainer) to fine-tune your model on the Rotten Tomatoes training split. Setting up the trainer will involve (at least):\n",
    "\n",
    "\n",
    "1. Instantiating a [`DataCollatorWithPadding`](https://huggingface.co/docs/transformers/en/main_classes/data_collator) object which is what *actually* does your batch construction (by padding all sequences to the same length).\n",
    "2. Writing an *evaluation function* that will measure the classification accuracy. This function takes a single argument which is a tuple containing `(logits, labels)` which you should use to compute classification accuracy (and maybe other metrics like F1 score, precision, recall) and return a `dict` with these metrics.  \n",
    "3. Instantiating a [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.51.1/en/main_classes/trainer#transformers.TrainingArguments) object using some reasonable defaults.\n",
    "4. Instantiating a `Trainer` object using your train and validation splits, you data collator, and function to compute performance metrics.\n",
    "5. Calling `trainer.train()`, waiting, waiting some more, and then calling `trainer.evaluate()` to see how it did.\n",
    "\n",
    "**Tip**: When prototyping this laboratory I discovered the HuggingFace [Evaluate library](https://huggingface.co/docs/evaluate/en/index) which provides evaluation metrics. However I found it to have insufferable layers of abstraction and getting actual metrics computed. I suggest just using the Scikit-learn metrics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)  \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-30T21:06:37.563287Z",
     "start_time": "2025-08-30T21:06:37.534433100Z"
    }
   },
   "id": "ffd757d28eb0a1b2"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "average_tokens_across_devices is True but world size is 1. Setting it to False automatically.\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the Training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 8,530\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4,020\n",
      "  Number of trainable parameters = 66,955,010\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='4020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/4020 : < :, Epoch 0.01/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-134\n",
      "Configuration saved in ./results\\checkpoint-134\\config.json\n",
      "Model weights saved in ./results\\checkpoint-134\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-134\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-134\\special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-268\n",
      "Configuration saved in ./results\\checkpoint-268\\config.json\n",
      "Model weights saved in ./results\\checkpoint-268\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-268\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-268\\special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-402\n",
      "Configuration saved in ./results\\checkpoint-402\\config.json\n",
      "Model weights saved in ./results\\checkpoint-402\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-402\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-402\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-268] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-536\n",
      "Configuration saved in ./results\\checkpoint-536\\config.json\n",
      "Model weights saved in ./results\\checkpoint-536\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-536\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-536\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-402] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-670\n",
      "Configuration saved in ./results\\checkpoint-670\\config.json\n",
      "Model weights saved in ./results\\checkpoint-670\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-670\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-670\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-536] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-804\n",
      "Configuration saved in ./results\\checkpoint-804\\config.json\n",
      "Model weights saved in ./results\\checkpoint-804\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-804\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-804\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-134] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-938\n",
      "Configuration saved in ./results\\checkpoint-938\\config.json\n",
      "Model weights saved in ./results\\checkpoint-938\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-938\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-938\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-670] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-1072\n",
      "Configuration saved in ./results\\checkpoint-1072\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1072\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-1072\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1072\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-938] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-1206\n",
      "Configuration saved in ./results\\checkpoint-1206\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1206\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-1206\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1206\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-1072] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-1340\n",
      "Configuration saved in ./results\\checkpoint-1340\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1340\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-1340\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1340\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-1206] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-1474\n",
      "Configuration saved in ./results\\checkpoint-1474\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1474\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-1474\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1474\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-1340] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-1608\n",
      "Configuration saved in ./results\\checkpoint-1608\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1608\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-1608\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1608\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-1474] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-1742\n",
      "Configuration saved in ./results\\checkpoint-1742\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1742\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-1742\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1742\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-1608] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-1876\n",
      "Configuration saved in ./results\\checkpoint-1876\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1876\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-1876\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1876\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-1742] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-2010\n",
      "Configuration saved in ./results\\checkpoint-2010\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2010\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-2010\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2010\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-1876] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-2144\n",
      "Configuration saved in ./results\\checkpoint-2144\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2144\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-2144\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2144\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-2010] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-2278\n",
      "Configuration saved in ./results\\checkpoint-2278\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2278\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-2278\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2278\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-2144] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-2412\n",
      "Configuration saved in ./results\\checkpoint-2412\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2412\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-2412\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2412\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-2278] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-2546\n",
      "Configuration saved in ./results\\checkpoint-2546\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2546\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-2546\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2546\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-2412] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-2680\n",
      "Configuration saved in ./results\\checkpoint-2680\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2680\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-2680\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2680\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-2546] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-2814\n",
      "Configuration saved in ./results\\checkpoint-2814\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2814\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-2814\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2814\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-2680] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-2948\n",
      "Configuration saved in ./results\\checkpoint-2948\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2948\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-2948\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2948\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-2814] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-3082\n",
      "Configuration saved in ./results\\checkpoint-3082\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3082\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-3082\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-3082\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-2948] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-3216\n",
      "Configuration saved in ./results\\checkpoint-3216\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3216\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-3216\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-3216\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-3082] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-3350\n",
      "Configuration saved in ./results\\checkpoint-3350\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3350\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-3350\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-3350\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-3216] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-3484\n",
      "Configuration saved in ./results\\checkpoint-3484\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3484\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-3484\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-3484\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-3350] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-3618\n",
      "Configuration saved in ./results\\checkpoint-3618\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3618\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-3618\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-3618\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-3484] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-3752\n",
      "Configuration saved in ./results\\checkpoint-3752\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3752\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-3752\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-3752\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-3618] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-3886\n",
      "Configuration saved in ./results\\checkpoint-3886\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3886\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-3886\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-3886\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-3752] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-4020\n",
      "Configuration saved in ./results\\checkpoint-4020\\config.json\n",
      "Model weights saved in ./results\\checkpoint-4020\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-4020\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-4020\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-3886] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results\\checkpoint-804 (score: 0.8470919324577861).\n",
      "Deleting older checkpoint [results\\checkpoint-4020] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=4020, training_loss=0.0009202774501549112, metrics={'train_runtime': 3209.7005, 'train_samples_per_second': 79.727, 'train_steps_per_second': 1.252, 'total_flos': 3698470624796808.0, 'train_loss': 0.0009202774501549112, 'epoch': 30.0})"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    logging_dir=\"./logs\",\n",
    "    eval_strategy=\"epoch\",      \n",
    "    save_strategy=\"epoch\",  \n",
    "    save_total_limit=1,           \n",
    "    load_best_model_at_end=True,   \n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=30,\n",
    "    logging_strategy=\"epoch\" \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-30T22:00:28.334001400Z",
     "start_time": "2025-08-30T21:06:57.653435300Z"
    }
   },
   "id": "853ddfbb494f78bb"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1066\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/plain": "   eval_loss  eval_accuracy  eval_precision  eval_recall   eval_f1  \\\n0    1.55037       0.847092        0.855769     0.834897  0.845204   \n\n   eval_runtime  eval_samples_per_second  eval_steps_per_second  epoch  \n0        2.9078                  366.598                  5.846   30.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eval_loss</th>\n      <th>eval_accuracy</th>\n      <th>eval_precision</th>\n      <th>eval_recall</th>\n      <th>eval_f1</th>\n      <th>eval_runtime</th>\n      <th>eval_samples_per_second</th>\n      <th>eval_steps_per_second</th>\n      <th>epoch</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.55037</td>\n      <td>0.847092</td>\n      <td>0.855769</td>\n      <td>0.834897</td>\n      <td>0.845204</td>\n      <td>2.9078</td>\n      <td>366.598</td>\n      <td>5.846</td>\n      <td>30.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "\n",
    "df_results = pd.DataFrame([results])\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T00:12:54.807037500Z",
     "start_time": "2025-08-31T00:12:51.821064600Z"
    }
   },
   "id": "a970711be1459b1e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 3.2: Fine-tuning a CLIP Model (harder)\n",
    "\n",
    "Use a (small) CLIP model like [`openai/clip-vit-base-patch16`](https://huggingface.co/openai/clip-vit-base-patch16) and evaluate its zero-shot performance on a small image classification dataset like ImageNette or TinyImageNet. Fine-tune (using a parameter-efficient method!) the CLIP model to see how much improvement you can squeeze out of it.\n",
    "\n",
    "**Note**: There are several ways to adapt the CLIP model; you could fine-tune the image encoder, the text encoder, or both. Or, you could experiment with prompt learning.\n",
    "\n",
    "**Tip**: CLIP probably already works very well on ImageNet and ImageNet-like images. For extra fun, look for an image classification dataset with different image types (e.g. *sketches*)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5eeca737-ee00-4d98-a3ae-f4d6eb3d264f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00b59ec2-4fe6-44c6-ab0b-0069f486bbb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T13:24:59.167297400Z",
     "start_time": "2025-08-31T13:24:56.990252100Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"zh-plus/tiny-imagenet\", split=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: goldfish\n",
      "1: European fire salamander\n",
      "2: bullfrog\n",
      "3: tailed frog\n",
      "4: American alligator\n",
      "5: boa constrictor\n",
      "6: trilobite\n",
      "7: scorpion\n",
      "8: black widow\n",
      "9: tarantula\n"
     ]
    }
   ],
   "source": [
    "words_file = 'words.txt'  # to map class names into human understandable labels\n",
    "wnid_to_name = {}\n",
    "\n",
    "with open(words_file) as f:\n",
    "    for line in f:\n",
    "        wnid, name = line.strip().split(\"\\t\")\n",
    "        wnid_to_name[wnid] = name.split(\",\")[0]  # to take only the first description, example  black widow, Latrodectus mactans -> black widow\n",
    "\n",
    "idx_to_class_name = {\n",
    "    i: wnid_to_name[wnid]\n",
    "    for i, wnid in enumerate(dataset.features[\"label\"].names)\n",
    "}\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"{i}: {idx_to_class_name[i]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:25:00.824238300Z",
     "start_time": "2025-08-31T13:25:00.730348800Z"
    }
   },
   "id": "9e170df254843859"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_image_with_label(idx, save=False):\n",
    "    item = dataset[idx]\n",
    "    image = item[\"image\"]\n",
    "    label_idx = item[\"label\"]\n",
    "    label_name = idx_to_class_name.get(label_idx, f\"Unknown ({label_idx})\")\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Label: {label_name}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    if save:\n",
    "        image.save(\"image.jpg\")\n",
    "\n",
    "\n",
    "show_image_with_label(32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:25:04.244491300Z",
     "start_time": "2025-08-31T13:25:04.149164300Z"
    }
   },
   "id": "184c021ac7635337"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04c768da5a87492cbbfa031f7159edab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"openai/clip-vit-base-patch16\"\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:36:14.649024700Z",
     "start_time": "2025-08-31T13:36:10.609983200Z"
    }
   },
   "id": "b90c03b88d27e346"
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 predictions:\n",
      "1: goldfish -> 0.776\n",
      "2: brain coral -> 0.066\n",
      "3: coral reef -> 0.055\n",
      "4: dugong -> 0.013\n",
      "5: reel -> 0.009\n"
     ]
    }
   ],
   "source": [
    "def show_image_with_clip_prediction(idx, topk=5, save=False):\n",
    "    item = dataset[idx]\n",
    "    image = item[\"image\"]\n",
    "    label_idx = item[\"label\"]\n",
    "    label_name = idx_to_class_name[label_idx]\n",
    "    \n",
    "\n",
    "    class_labels = [idx_to_class_name[i] for i in range(len(idx_to_class_name))]\n",
    "    text_inputs = processor(text=class_labels, return_tensors=\"pt\", padding=True).to(device)\n",
    "    image_input = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**image_input, **text_inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        probs = logits_per_image.softmax(dim=-1)\n",
    "        pred_idx = logits_per_image.argmax().item()\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"True: {label_name} | Predicted: {idx_to_class_name[pred_idx]}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    top_probs, top_idx = probs.topk(topk)\n",
    "    print(f\"\\nTop-{topk} predictions:\")\n",
    "    for i, (p, j) in enumerate(zip(top_probs[0], top_idx[0])):\n",
    "        print(f\"{i+1}: {idx_to_class_name[j.item()]} -> {p.item():.3f}\")\n",
    "    \n",
    "    if save:\n",
    "        image.save(\"image_pred.jpg\")\n",
    "\n",
    "show_image_with_clip_prediction(32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T12:32:32.118564Z",
     "start_time": "2025-08-31T12:32:31.700365800Z"
    }
   },
   "id": "a43b0f5b2ed8826e"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "n_val = 100\n",
    "n_train = 500 \n",
    "seed = 29\n",
    "\n",
    "\n",
    "random.seed(seed)\n",
    "all_indices = list(range(len(dataset)))\n",
    "\n",
    "val_indices = random.sample(all_indices, n_val)\n",
    "\n",
    "remaining_indices = list(set(all_indices) - set(val_indices))\n",
    "train_indices = random.sample(remaining_indices, n_train)\n",
    "\n",
    "train_subset = [dataset[i] for i in train_indices]\n",
    "val_subset = [dataset[i] for i in val_indices]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:25:55.791316900Z",
     "start_time": "2025-08-31T13:25:55.634272Z"
    }
   },
   "id": "ca5f688d09cc53e0"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class_labels = [idx_to_class_name[i] for i in range(len(idx_to_class_name))]\n",
    "text_inputs = processor(text=class_labels, return_tensors=\"pt\", padding=True).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:25:59.335785100Z",
     "start_time": "2025-08-31T13:25:59.302192700Z"
    }
   },
   "id": "62a4483cd6f96989"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-shot evaluation: 100%|██████████| 100/100 [00:06<00:00, 15.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot accuracy on 100 validation samples: 0.6300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for item in tqdm(val_subset, desc=\"Zero-shot evaluation\"):\n",
    "    image = processor(images=item[\"image\"], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**image, **text_inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        pred_idx = logits_per_image.argmax().item()\n",
    "    \n",
    "    if pred_idx == item[\"label\"]:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / n_val\n",
    "print(f\"Zero-shot accuracy on {n_val} validation samples: {accuracy:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:25:36.779066300Z",
     "start_time": "2025-08-31T13:25:30.269548900Z"
    }
   },
   "id": "aff1d477c3f7af1e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's try to add \"a photo of a ...\" to the text inputs, to have a simple baseline for the fine tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2ec75bf55f5600d"
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-shot evaluation with prompts: 100%|██████████| 100/100 [00:07<00:00, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot accuracy with 'a photo of a ...' prompts on 100 validation samples: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompted_class_labels = [f\"a photo of a {label}\" for label in class_labels]\n",
    "text_inputs = processor(text=prompted_class_labels, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for item in tqdm(val_subset, desc=\"Zero-shot evaluation with prompts\"):\n",
    "    image_input = processor(images=item[\"image\"], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**image_input, **text_inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        pred_idx = logits_per_image.argmax().item()\n",
    "    \n",
    "    if pred_idx == item[\"label\"]:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / n_val\n",
    "print(f\"Zero-shot accuracy with 'a photo of a ...' prompts on {n_val} validation samples: {accuracy:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T12:36:44.164389Z",
     "start_time": "2025-08-31T12:36:36.407636600Z"
    }
   },
   "id": "721e739807b6b7f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fine tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c585d2f1f6028d"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoder: \n",
      "\n",
      " CLIPTextTransformer(\n",
      "  (embeddings): CLIPTextEmbeddings(\n",
      "    (token_embedding): Embedding(49408, 512)\n",
      "    (position_embedding): Embedding(77, 512)\n",
      "  )\n",
      "  (encoder): CLIPEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x CLIPEncoderLayer(\n",
      "        (self_attn): CLIPAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): CLIPMLP(\n",
      "          (activation_fn): QuickGELUActivation()\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "embeddings CLIPTextEmbeddings(\n",
      "  (token_embedding): Embedding(49408, 512)\n",
      "  (position_embedding): Embedding(77, 512)\n",
      ")\n",
      "embeddings.token_embedding Embedding(49408, 512)\n",
      "embeddings.position_embedding Embedding(77, 512)\n",
      "encoder CLIPEncoder(\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x CLIPEncoderLayer(\n",
      "      (self_attn): CLIPAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): CLIPMLP(\n",
      "        (activation_fn): QuickGELUActivation()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "encoder.layers ModuleList(\n",
      "  (0-11): 12 x CLIPEncoderLayer(\n",
      "    (self_attn): CLIPAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): CLIPMLP(\n",
      "      (activation_fn): QuickGELUActivation()\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "encoder.layers.0 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.0.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.0.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.0.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.0.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.0.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.0.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.0.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.0.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.0.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n",
      "encoder.layers.0.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n",
      "encoder.layers.0.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.1 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.1.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.1.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.1.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.1.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.1.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.1.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.1.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.1.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.1.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n",
      "encoder.layers.1.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n",
      "encoder.layers.1.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.2 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.2.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.2.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.2.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.2.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.2.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.2.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.2.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.2.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.2.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n",
      "encoder.layers.2.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n",
      "encoder.layers.2.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.3 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.3.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.3.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.3.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.3.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.3.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.3.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.3.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.3.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.3.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n",
      "encoder.layers.3.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n",
      "encoder.layers.3.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.4 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.4.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.4.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.4.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.4.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.4.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.4.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.4.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.4.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.4.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n",
      "encoder.layers.4.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n",
      "encoder.layers.4.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.5 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.5.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.5.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.5.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.5.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.5.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.5.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.5.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.5.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.5.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n",
      "encoder.layers.5.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n",
      "encoder.layers.5.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.6 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.6.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.6.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.6.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.6.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.6.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.6.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.6.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.6.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.6.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n",
      "encoder.layers.6.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n",
      "encoder.layers.6.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.7 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.7.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.7.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.7.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.7.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.7.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.7.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.7.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.7.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.7.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n",
      "encoder.layers.7.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n",
      "encoder.layers.7.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.8 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.8.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.8.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.8.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.8.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.8.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.8.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.8.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.8.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.8.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n",
      "encoder.layers.8.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n",
      "encoder.layers.8.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.9 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.9.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.9.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.9.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.9.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.9.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.9.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.9.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.9.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.9.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n",
      "encoder.layers.9.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n",
      "encoder.layers.9.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.10 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.10.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.10.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.10.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.10.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.10.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.10.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.10.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.10.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.10.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n",
      "encoder.layers.10.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n",
      "encoder.layers.10.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.11 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.11.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.11.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.11.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.11.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.11.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n",
      "encoder.layers.11.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.11.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n",
      "encoder.layers.11.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.11.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n",
      "encoder.layers.11.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n",
      "encoder.layers.11.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "final_layer_norm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Text encoder: \\n\")\n",
    "for name, module in model.text_model.named_modules():\n",
    "    print(name, module)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:26:06.272765300Z",
     "start_time": "2025-08-31T13:26:06.236652300Z"
    }
   },
   "id": "a29ca7e2ff6a076c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### layers are like: encoder.layers.11.self_attn.k_proj, encoder.layers.11.self_attn.v_proj, encoder.layers.11.self_attn.q_proj \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86f23ec7b873c1fb"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image encoder: \n",
      "\n",
      " CLIPVisionTransformer(\n",
      "  (embeddings): CLIPVisionEmbeddings(\n",
      "    (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "    (position_embedding): Embedding(197, 768)\n",
      "  )\n",
      "  (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder): CLIPEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x CLIPEncoderLayer(\n",
      "        (self_attn): CLIPAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): CLIPMLP(\n",
      "          (activation_fn): QuickGELUActivation()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "embeddings CLIPVisionEmbeddings(\n",
      "  (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "  (position_embedding): Embedding(197, 768)\n",
      ")\n",
      "embeddings.patch_embedding Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "embeddings.position_embedding Embedding(197, 768)\n",
      "pre_layrnorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder CLIPEncoder(\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x CLIPEncoderLayer(\n",
      "      (self_attn): CLIPAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): CLIPMLP(\n",
      "        (activation_fn): QuickGELUActivation()\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "encoder.layers ModuleList(\n",
      "  (0-11): 12 x CLIPEncoderLayer(\n",
      "    (self_attn): CLIPAttention(\n",
      "      (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): CLIPMLP(\n",
      "      (activation_fn): QuickGELUActivation()\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "encoder.layers.0 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.0.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.0.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.0.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.0.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.0.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.0.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.0.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.0.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.0.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n",
      "encoder.layers.0.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n",
      "encoder.layers.0.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.1 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.1.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.1.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.1.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.1.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.1.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.1.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.1.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.1.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.1.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n",
      "encoder.layers.1.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n",
      "encoder.layers.1.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.2 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.2.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.2.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.2.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.2.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.2.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.2.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.2.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.2.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.2.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n",
      "encoder.layers.2.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n",
      "encoder.layers.2.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.3 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.3.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.3.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.3.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.3.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.3.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.3.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.3.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.3.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.3.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n",
      "encoder.layers.3.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n",
      "encoder.layers.3.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.4 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.4.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.4.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.4.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.4.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.4.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.4.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.4.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.4.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.4.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n",
      "encoder.layers.4.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n",
      "encoder.layers.4.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.5 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.5.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.5.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.5.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.5.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.5.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.5.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.5.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.5.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.5.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n",
      "encoder.layers.5.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n",
      "encoder.layers.5.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.6 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.6.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.6.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.6.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.6.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.6.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.6.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.6.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.6.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.6.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n",
      "encoder.layers.6.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n",
      "encoder.layers.6.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.7 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.7.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.7.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.7.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.7.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.7.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.7.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.7.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.7.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.7.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n",
      "encoder.layers.7.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n",
      "encoder.layers.7.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.8 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.8.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.8.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.8.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.8.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.8.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.8.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.8.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.8.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.8.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n",
      "encoder.layers.8.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n",
      "encoder.layers.8.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.9 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.9.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.9.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.9.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.9.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.9.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.9.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.9.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.9.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.9.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n",
      "encoder.layers.9.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n",
      "encoder.layers.9.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.10 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.10.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.10.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.10.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.10.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.10.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.10.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.10.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.10.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.10.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n",
      "encoder.layers.10.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n",
      "encoder.layers.10.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.11 CLIPEncoderLayer(\n",
      "  (self_attn): CLIPAttention(\n",
      "    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): CLIPMLP(\n",
      "    (activation_fn): QuickGELUActivation()\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.layers.11.self_attn CLIPAttention(\n",
      "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.11.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.11.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.11.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.11.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n",
      "encoder.layers.11.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.layers.11.mlp CLIPMLP(\n",
      "  (activation_fn): QuickGELUActivation()\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "encoder.layers.11.mlp.activation_fn QuickGELUActivation()\n",
      "encoder.layers.11.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n",
      "encoder.layers.11.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n",
      "encoder.layers.11.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "post_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Image encoder: \\n\")\n",
    "for name, module in model.vision_model.named_modules():\n",
    "    print(name, module)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:27:46.805992400Z",
     "start_time": "2025-08-31T13:27:46.772165500Z"
    }
   },
   "id": "5757cbe715dfa13b"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 737,280 || all params: 150,358,017 || trainable%: 0.4903\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],  \n",
    "    task_type=\"FEATURE_EXTRACTION\" \n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:38:25.824989400Z",
     "start_time": "2025-08-31T13:38:25.670292400Z"
    }
   },
   "id": "ca05eaec16a59dc7"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 500/500 [01:21<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average training loss: 1.6215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation: 100%|██████████| 100/100 [00:07<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation accuracy: 0.7100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 500/500 [01:21<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average training loss: 0.6694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation: 100%|██████████| 100/100 [00:07<00:00, 13.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 validation accuracy: 0.7200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 500/500 [01:35<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average training loss: 0.2744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation: 100%|██████████| 100/100 [00:09<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 validation accuracy: 0.7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 500/500 [01:48<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 average training loss: 0.0879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation: 100%|██████████| 100/100 [00:09<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 validation accuracy: 0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 500/500 [01:50<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 average training loss: 0.0265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation: 100%|██████████| 100/100 [00:10<00:00,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 validation accuracy: 0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    running_loss = 0\n",
    "    for item in tqdm(train_subset, desc=f\"Epoch {epoch+1} Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        image_input = processor(images=item[\"image\"], return_tensors=\"pt\").pixel_values.to(device)\n",
    "        image_features = model.get_image_features(pixel_values=image_input)\n",
    "        \n",
    "        text_inputs = processor(text=class_labels, return_tensors=\"pt\", padding=True).to(device)\n",
    "        text_features = model.get_text_features(input_ids=text_inputs.input_ids,\n",
    "                                                attention_mask=text_inputs.attention_mask)\n",
    "        \n",
    "\n",
    "        logits = image_features @ text_features.T\n",
    "        labels = torch.tensor([item[\"label\"]]).to(device)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_subset)\n",
    "    print(f\"Epoch {epoch+1} average training loss: {avg_loss:.4f}\")\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(val_subset, desc=f\"Epoch {epoch+1} Validation\"):\n",
    "            image_input = processor(images=item[\"image\"], return_tensors=\"pt\").pixel_values.to(device)\n",
    "            text_inputs = processor(text=class_labels, return_tensors=\"pt\", padding=True).to(device)\n",
    "            \n",
    "            image_features = model.get_image_features(pixel_values=image_input)\n",
    "            text_features = model.get_text_features(\n",
    "                input_ids=text_inputs.input_ids,\n",
    "                attention_mask=text_inputs.attention_mask\n",
    "            )\n",
    "            logits = image_features @ text_features.T\n",
    "            pred_idx = logits.argmax().item()\n",
    "            if pred_idx == item[\"label\"]:\n",
    "                correct += 1\n",
    "\n",
    "    val_accuracy = correct / len(val_subset)\n",
    "    print(f\"Epoch {epoch+1} validation accuracy: {val_accuracy:.4f}\")\n",
    "    model.train()  \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:51:45.058740900Z",
     "start_time": "2025-08-31T13:43:03.716902400Z"
    }
   },
   "id": "32a95e1638ce512c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### With fine tune (on both encoder) we achived higer accuracy, let's now see the differences on finetuning weights in the image or text decoder only"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b20ae14365a91f4"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9849e31c93444e6bb124699aaf49c98d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 150,358,017 || trainable%: 0.1961\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openai/clip-vit-base-patch16\"\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],  \n",
    "    task_type=\"FEATURE_EXTRACTION\" \n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "for param in model.vision_model.parameters():\n",
    "    param.requires_grad = False    # only fine tune text encoder\n",
    "    \n",
    "model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:56:44.407120700Z",
     "start_time": "2025-08-31T13:56:38.207421700Z"
    }
   },
   "id": "c791d667c1a7fe93"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 500/500 [01:09<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average training loss: 1.8706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation: 100%|██████████| 100/100 [00:08<00:00, 12.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation accuracy: 0.6800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 500/500 [01:12<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average training loss: 1.3117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation: 100%|██████████| 100/100 [00:08<00:00, 12.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 validation accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 500/500 [01:29<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average training loss: 1.1145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation: 100%|██████████| 100/100 [00:10<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 validation accuracy: 0.7100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 500/500 [01:32<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 average training loss: 0.9486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation: 100%|██████████| 100/100 [00:09<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 validation accuracy: 0.7300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 500/500 [01:33<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 average training loss: 0.7926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation: 100%|██████████| 100/100 [00:10<00:00,  9.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 validation accuracy: 0.7300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    running_loss = 0\n",
    "    for item in tqdm(train_subset, desc=f\"Epoch {epoch+1} Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        image_input = processor(images=item[\"image\"], return_tensors=\"pt\").pixel_values.to(device)\n",
    "        image_features = model.get_image_features(pixel_values=image_input)\n",
    "        \n",
    "        text_inputs = processor(text=class_labels, return_tensors=\"pt\", padding=True).to(device)\n",
    "        text_features = model.get_text_features(input_ids=text_inputs.input_ids,\n",
    "                                                attention_mask=text_inputs.attention_mask)\n",
    "        \n",
    "\n",
    "        logits = image_features @ text_features.T\n",
    "        labels = torch.tensor([item[\"label\"]]).to(device)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_subset)\n",
    "    print(f\"Epoch {epoch+1} average training loss: {avg_loss:.4f}\")\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(val_subset, desc=f\"Epoch {epoch+1} Validation\"):\n",
    "            image_input = processor(images=item[\"image\"], return_tensors=\"pt\").pixel_values.to(device)\n",
    "            text_inputs = processor(text=class_labels, return_tensors=\"pt\", padding=True).to(device)\n",
    "            \n",
    "            image_features = model.get_image_features(pixel_values=image_input)\n",
    "            text_features = model.get_text_features(\n",
    "                input_ids=text_inputs.input_ids,\n",
    "                attention_mask=text_inputs.attention_mask\n",
    "            )\n",
    "            logits = image_features @ text_features.T\n",
    "            pred_idx = logits.argmax().item()\n",
    "            if pred_idx == item[\"label\"]:\n",
    "                correct += 1\n",
    "\n",
    "    val_accuracy = correct / len(val_subset)\n",
    "    print(f\"Epoch {epoch+1} validation accuracy: {val_accuracy:.4f}\")\n",
    "    model.train()  \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T14:04:48.528489800Z",
     "start_time": "2025-08-31T13:57:04.016179100Z"
    }
   },
   "id": "2d3bb1a6c8088b2"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "25e2af6cdf094e5fac983096aa659670"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 442,368 || all params: 150,358,017 || trainable%: 0.2942\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openai/clip-vit-base-patch16\"\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],  \n",
    "    task_type=\"FEATURE_EXTRACTION\" \n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "for param in model.text_model.parameters():\n",
    "    param.requires_grad = False    # only fine tune vision encoder\n",
    "    \n",
    "model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T14:06:47.612533300Z",
     "start_time": "2025-08-31T14:06:42.257995100Z"
    }
   },
   "id": "39951e01881aa836"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 500/500 [01:03<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average training loss: 1.8970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation: 100%|██████████| 100/100 [00:08<00:00, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation accuracy: 0.7200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 500/500 [01:05<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average training loss: 0.9143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation: 100%|██████████| 100/100 [00:08<00:00, 12.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 validation accuracy: 0.6800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 500/500 [01:02<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average training loss: 0.4703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation: 100%|██████████| 100/100 [00:08<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 validation accuracy: 0.6700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 500/500 [01:12<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 average training loss: 0.2149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation: 100%|██████████| 100/100 [00:10<00:00,  9.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 validation accuracy: 0.7100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 500/500 [01:12<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 average training loss: 0.0888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation: 100%|██████████| 100/100 [00:10<00:00,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 validation accuracy: 0.6800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    running_loss = 0\n",
    "    for item in tqdm(train_subset, desc=f\"Epoch {epoch+1} Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        image_input = processor(images=item[\"image\"], return_tensors=\"pt\").pixel_values.to(device)\n",
    "        image_features = model.get_image_features(pixel_values=image_input)\n",
    "        \n",
    "        text_inputs = processor(text=class_labels, return_tensors=\"pt\", padding=True).to(device)\n",
    "        text_features = model.get_text_features(input_ids=text_inputs.input_ids,\n",
    "                                                attention_mask=text_inputs.attention_mask)\n",
    "        \n",
    "\n",
    "        logits = image_features @ text_features.T\n",
    "        labels = torch.tensor([item[\"label\"]]).to(device)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_subset)\n",
    "    print(f\"Epoch {epoch+1} average training loss: {avg_loss:.4f}\")\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(val_subset, desc=f\"Epoch {epoch+1} Validation\"):\n",
    "            image_input = processor(images=item[\"image\"], return_tensors=\"pt\").pixel_values.to(device)\n",
    "            text_inputs = processor(text=class_labels, return_tensors=\"pt\", padding=True).to(device)\n",
    "            \n",
    "            image_features = model.get_image_features(pixel_values=image_input)\n",
    "            text_features = model.get_text_features(\n",
    "                input_ids=text_inputs.input_ids,\n",
    "                attention_mask=text_inputs.attention_mask\n",
    "            )\n",
    "            logits = image_features @ text_features.T\n",
    "            pred_idx = logits.argmax().item()\n",
    "            if pred_idx == item[\"label\"]:\n",
    "                correct += 1\n",
    "\n",
    "    val_accuracy = correct / len(val_subset)\n",
    "    print(f\"Epoch {epoch+1} validation accuracy: {val_accuracy:.4f}\")\n",
    "    model.train()  \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T14:13:41.083896300Z",
     "start_time": "2025-08-31T14:07:19.288690500Z"
    }
   },
   "id": "c3102f6a8e534854"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### https://www.kaggle.com/code/jaafaryassine/art-style-classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70842409a9e5d4ea"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "           artist    date     genre  pixelsx  pixelsy  size_bytes   source  \\\n0  Barnett Newman  1955.0  abstract  15530.0   6911.0   9201912.0  wikiart   \n1  Barnett Newman  1950.0  abstract  14559.0   6866.0   8867532.0  wikiart   \n2     kiri nichol  2013.0       NaN   9003.0   9004.0   1756681.0      NaN   \n3     kiri nichol  2013.0       NaN   9003.0   9004.0   1942046.0      NaN   \n4     kiri nichol  2013.0       NaN   9003.0   9004.0   1526212.0      NaN   \n\n                  style                  title    filename  \n0  Color Field Painting                  Uriel  102257.jpg  \n1  Color Field Painting  Vir Heroicus Sublimis   75232.jpg  \n2         Neoplasticism                    NaN   32145.jpg  \n3         Neoplasticism                    NaN   20304.jpg  \n4         Neoplasticism                    NaN     836.jpg  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist</th>\n      <th>date</th>\n      <th>genre</th>\n      <th>pixelsx</th>\n      <th>pixelsy</th>\n      <th>size_bytes</th>\n      <th>source</th>\n      <th>style</th>\n      <th>title</th>\n      <th>filename</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Barnett Newman</td>\n      <td>1955.0</td>\n      <td>abstract</td>\n      <td>15530.0</td>\n      <td>6911.0</td>\n      <td>9201912.0</td>\n      <td>wikiart</td>\n      <td>Color Field Painting</td>\n      <td>Uriel</td>\n      <td>102257.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Barnett Newman</td>\n      <td>1950.0</td>\n      <td>abstract</td>\n      <td>14559.0</td>\n      <td>6866.0</td>\n      <td>8867532.0</td>\n      <td>wikiart</td>\n      <td>Color Field Painting</td>\n      <td>Vir Heroicus Sublimis</td>\n      <td>75232.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>kiri nichol</td>\n      <td>2013.0</td>\n      <td>NaN</td>\n      <td>9003.0</td>\n      <td>9004.0</td>\n      <td>1756681.0</td>\n      <td>NaN</td>\n      <td>Neoplasticism</td>\n      <td>NaN</td>\n      <td>32145.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>kiri nichol</td>\n      <td>2013.0</td>\n      <td>NaN</td>\n      <td>9003.0</td>\n      <td>9004.0</td>\n      <td>1942046.0</td>\n      <td>NaN</td>\n      <td>Neoplasticism</td>\n      <td>NaN</td>\n      <td>20304.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>kiri nichol</td>\n      <td>2013.0</td>\n      <td>NaN</td>\n      <td>9003.0</td>\n      <td>9004.0</td>\n      <td>1526212.0</td>\n      <td>NaN</td>\n      <td>Neoplasticism</td>\n      <td>NaN</td>\n      <td>836.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art = pd.read_csv('art_dataset/imagesinfo.csv')\n",
    "art.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-31T14:21:46.464219600Z",
     "start_time": "2025-08-31T14:21:46.113810600Z"
    }
   },
   "id": "fdd5efc45fa465c2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
